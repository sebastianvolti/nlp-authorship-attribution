{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte A - Palabras más frecuentes, ley de Zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte se pide, para cada texto, tokenizarlos, calcular las frecuencias y ordenarlos.\n",
    "\n",
    "En primer lugar se deben importar e instalar las librerías correspondientes del siguiente bloque de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presentan dos funciones. La primera, `tokenize_text`, que utiliza la función `word_tokenize` de nltk para tokenizar el texto de un archivo cuyo path se pasa por parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text_path):\n",
    "    with open(text_path, 'r') as text:\n",
    "        return word_tokenize(text.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda, `get_frecuencies`, que utiliza la clase `FreqDist` de nltk para obtener las frecuencias de cada palabra. Luego se ordenan dichas frecuencias de forma descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_frecuencies(tokenized_text):\n",
    "    return FreqDist(word.lower() for word in tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando las dos funciones anteriores se obtiene, para cada autor, una lista con los textos tokenizados y ordenados de forma descendente. Se imprimen los 10 primeros elementos de cada lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6049, 5437, 5320, 3243, 2392, 2364, 2187, 2043, 1758, 1621],\n",
      " [10673, 6581, 6302, 4347, 3426, 3222, 2770, 2685, 2234, 2161],\n",
      " [21114, 13428, 9996, 8117, 7129, 6417, 5874, 4992, 4923, 3726],\n",
      " [9115, 8583, 8457, 6240, 5253, 4909, 4634, 3503, 3295, 3055],\n",
      " [13213, 8758, 8533, 4970, 3756, 3609, 3269, 3236, 3148, 2677]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "authors = ['L1-Conrad', 'L2-Zola', 'L3-Proust', 'L4-Austen', 'L5-Flaubert']\n",
    "frecuencies = {}\n",
    "\n",
    "for i, author in enumerate(authors):\n",
    "    tokenized_text = tokenize_text(f'textos/ConAutor/{author}.txt')\n",
    "    frecuency_values = sorted([x for x in get_frecuencies(tokenized_text).values()], reverse=True)\n",
    "    frecuencies[author] = frecuency_values\n",
    "\n",
    "pprint.pprint([fv[:10] for fv in frecuencies.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ley de Zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Ley de Zipf [1] propone que, dado un corpus de texto, hay una relación matemática aproximada entre la frecuencia de ocurrencia de cada palabra, y el rango de cada una de ellas en la lista de todas las palabras usadas en el texto ordenada de forma descendente según la frecuencia. El rango corresponde a la posición de la palabra en dicha lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea un identificador *s* para cada palabra que corresponde a su rango, y *f(s)* la frecuencia relativa de ocurrencias de dicha palabra, entonces la Ley de Zipf propone que la siguiente relación se mantiene de forma aproximada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(s) = \\frac{A}{s^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde el exponente *x* toma un valor ligeramente mayor a 1, y *A* es una constante utilizada para normalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto implica que el segundo elemento se repetirá aproximadamente con una frecuencia de 1/2 de la del primero, el tercer elemento con una frecuencia de 1/3 del primero y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrección de Zipf-Mandelbrot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Corrección de Zipf-Mandelbrot [1] utiliza argumentos sobre la estructura fractal de los árboles léxicos. La fórmula génerica descrita por Mandelbrot puede ser escrita de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(s) = \\frac{A}{(1 + C.s)^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde *C* es un segundo parámetro que requiere ser modificado para ajustarse a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas leyes son utilizadas en un amplio espectro de ámbitos además de la linguística (medicina, deporte, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfica frecuencias vs rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(data):\n",
    "    plots = []\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    for tokens_ranges, frecuencies, label in data:\n",
    "        plots.append(plt.plot(tokens_ranges, frecuencies, label=label)[0])\n",
    "\n",
    "    plt.legend(handles=plots)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots = []\n",
    "for i, text in enumerate(authors):\n",
    "    plots.append((list(range(len(frecuency_values))), frecuency_values, authors[i]))\n",
    "\n",
    "plot(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Palabras con una aparición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se listan las frecuencias absolutas y relativas de las palabras que aparecen una única vez en cada libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     L1-Conrad L2-Zola L3-Proust L4-Austen L5-Flaubert\n",
      "absolute_frecuencies      7258    7516     11656      7405        9298\n",
      "relative_frecuencies      0.57    0.51      0.54      0.52        0.53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "absolute_frecuencies = []\n",
    "relative_frecuencies = []\n",
    "\n",
    "for author_frecuencies in frecuencies.values():\n",
    "    count = len(list(filter(lambda x: x == 1, author_frecuencies)))\n",
    "    absolute_frecuencies.append(count)\n",
    "    relative_frecuencies.append(\"{0:.2f}\".format(count/len(author_frecuencies)))\n",
    "\n",
    "table = {\n",
    "    'absolute_frecuencies': absolute_frecuencies,\n",
    "    'relative_frecuencies': relative_frecuencies\n",
    "}\n",
    "dfObj = pd.DataFrame.from_dict(table, orient='index', columns=authors) \n",
    "print(dfObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte B - Distancia Delta entre textos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente generamos el conjunto de funciones necesarias para el modelo delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def Fij(j,i,ocurr_texts):    \n",
    "    return ocurr_texts[j][i]\n",
    "\n",
    "def Dj(j,ocurr_texts):\n",
    "    sum=0\n",
    "    for i in range(150):\n",
    "        sum+=Fij(j,i,ocurr_texts)\n",
    "    return sum\n",
    "    \n",
    "\n",
    "def FRij(j,i,ocurr_texts):\n",
    "    return Fij(j,i,ocurr_texts)/Dj(j,ocurr_texts)\n",
    "\n",
    "\n",
    "def media(i,ocurr_texts):\n",
    "    FR=[]\n",
    "    for j in range(5):\n",
    "        FR.append(FRij(j,i,ocurr_texts))\n",
    "    return statistics.mean(FR)\n",
    "    \n",
    "    \n",
    "\n",
    "def desviacioni(i,ocurr_texts):\n",
    "    FR=[]\n",
    "    for j in range(5):\n",
    "        FR.append(FRij(j,i,ocurr_texts))\n",
    "    return statistics.stdev(FR)\n",
    "\n",
    "\n",
    "def Zij(j,i,ocurr_texts):\n",
    "    if desviacioni(i,ocurr_texts)==0: return 0\n",
    "    else: return (FRij(j,i,ocurr_texts)-media(i,ocurr_texts))/desviacioni(i,ocurr_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego para el procesamiento de los textos, generamos dos listas, una con los textos con autores que nos interesa perfilar y otra con los textos a determinar su ator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "L=[]\n",
    "L.append(open(\"textos/ConAutor/L1-Conrad.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "L.append(open(\"textos/ConAutor/L2-Zola.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "L.append(open(\"textos/ConAutor/L3-Proust.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "L.append(open(\"textos/ConAutor/L4-Austen.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "L.append(open(\"textos/ConAutor/L5-Flaubert.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "\n",
    "T=[]\n",
    "T.append(open(\"textos/SinAutor/T1.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "T.append(open(\"textos/SinAutor/T2.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "T.append(open(\"textos/SinAutor/T3.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "T.append(open(\"textos/SinAutor/T4.txt\", \"r\",encoding=\"latin_1\").read())\n",
    "T.append(open(\"textos/SinAutor/T5.txt\", \"r\",encoding=\"latin_1\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necesitaba determinar las 150 palabras mas comunes en la coleccion de textos con autor y para esto utilizamos la libreria llamda collections que contiene una funcion llamada Counter, la cual utilizamos para determinar la cantdad de ocurrencias de cada palabra y asi obetenr las 150 mas usadas.\n",
    "\n",
    "Luego hacemos una pequeña limpieza en el texto, eliminando los caracteres '\\n' y las puntuaciones.\n",
    "Por ultimo decidimos ordenar estas palabras alfabeticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set is the colleccion\n",
    "#i is a word (string) in the colleccion\n",
    "#j in a authorn (int) j=0 -> L1 , j=1 -> L2, j=2 -> L3, j=3 -> L4, j=4 -> L5\n",
    "\n",
    "data_set = L[0]+L[1]+L[2]+L[3]+L[4]\n",
    "text = re.sub('([\\W_]+)|(\\n)', ' ',data_set.lower())\n",
    "split_it = text.split()\n",
    "counter = Counter(split_it)\n",
    "most_occur = counter.most_common(150)\n",
    "new_list = [ seq[0] for seq in most_occur ]\n",
    "order_vector=sorted(new_list)\n",
    "#order_vector has the n word most commun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar las ocurrencias de estas 150 palabras conocidas en cada texto de autor, generamos una lista ocurr_texts que tiene 5 posiciones y en doden cada posocion corresponde a un autor conocido.\n",
    "\n",
    "Dentro de cada posicion de la lista ocurr_texts guardamos otra lista que contiene, en el mismo orden alfabetico que fue determinado arriba la cantidad de ocurrencias de cada palabra de interes para cada autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocurr_texts=[]\n",
    "#List that contains the vector ocurrences for each author\n",
    "\n",
    "for i in range(5):\n",
    "    l = [0] * 150\n",
    "    text = re.sub('([\\W_]+)|(\\n)', ' ',L[i].lower())\n",
    "    split_it = text.split()\n",
    "    counter = Counter(split_it)\n",
    "    for i,elem in enumerate(order_vector):\n",
    "        if elem in counter.keys():\n",
    "            l[i]=(counter[elem])\n",
    "    ocurr_texts.append(l)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion perfilamos a los autores utilizando las funciones definidas para el modelo delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_conrad= [0] * 150\n",
    "profile_zola= [0] * 150\n",
    "profile_proust= [0] * 150\n",
    "profile_austen= [0] * 150\n",
    "profile_flaubert= [0] * 150\n",
    "\n",
    "#Profiling authors\n",
    "for i in range(150):\n",
    "    profile_conrad[i] = Zij(0,i,ocurr_texts)\n",
    "    profile_zola[i] = Zij(1,i,ocurr_texts)\n",
    "    profile_proust[i] = Zij(2,i,ocurr_texts)\n",
    "    profile_austen[i] = Zij(3,i,ocurr_texts)\n",
    "    profile_flaubert[i] = Zij(4,i,ocurr_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos los mismos pasos para los textos T1,T2,T3,T4 y T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocurr_texts_without_author=[]\n",
    "#List that contains the vector ocurrences for each instance without author\n",
    "\n",
    "for i in range(5):\n",
    "    t = [0] * 150\n",
    "    text = re.sub('([\\W_]+)|(\\n)', ' ',T[i].lower())\n",
    "    split_it = text.split()\n",
    "    counter = Counter(split_it)\n",
    "    for i,elem in enumerate(order_vector):\n",
    "        if elem in counter.keys():\n",
    "            t[i]=(counter[elem])\n",
    "    ocurr_texts_without_author.append(t)\n",
    "\n",
    "#Calculate best author\n",
    "profile_t1= [0] * 150\n",
    "profile_t2= [0] * 150\n",
    "profile_t3= [0] * 150\n",
    "profile_t4= [0] * 150\n",
    "profile_t5= [0] * 150\n",
    "\n",
    "#Profiling instance without author\n",
    "for i in range(150):\n",
    "    profile_t1[i] = Zij(0,i,ocurr_texts_without_author)\n",
    "    profile_t2[i] = Zij(1,i,ocurr_texts_without_author)\n",
    "    profile_t3[i] = Zij(2,i,ocurr_texts_without_author)\n",
    "    profile_t4[i] = Zij(3,i,ocurr_texts_without_author)\n",
    "    profile_t5[i] = Zij(4,i,ocurr_texts_without_author)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo para la evaluacion, creamos una funcion que calcula la dinstancia de manhattan para dos vectores y generamos una tabla con los resultaos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_manhattan(x,y):\n",
    "    sum=0\n",
    "    for i in range(150):\n",
    "        dist=abs(x[i]-y[i])\n",
    "        sum+=dist\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        conrad        zola      proust      austen     flauber\n",
      "T1  110.736950  159.047728  179.274339  170.176775  146.176080\n",
      "T2  169.606863   93.010435  162.126623  202.635320  111.804623\n",
      "T3  144.133734  185.893669   66.798071  166.736314  165.427579\n",
      "T4  152.786246  211.529657  181.007531   82.746270  194.193266\n",
      "T5  173.910190  116.585892  175.894724  177.692387   85.322346\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "distances_conrad=[distancia_manhattan(profile_conrad,profile_t1),distancia_manhattan(profile_conrad,profile_t2),distancia_manhattan(profile_conrad,profile_t3),distancia_manhattan(profile_conrad,profile_t4),distancia_manhattan(profile_conrad,profile_t5)]\n",
    "distances_zola=[distancia_manhattan(profile_zola,profile_t1),distancia_manhattan(profile_zola,profile_t2),distancia_manhattan(profile_zola,profile_t3),distancia_manhattan(profile_zola,profile_t4),distancia_manhattan(profile_zola,profile_t5)]\n",
    "distances_proust=[distancia_manhattan(profile_proust,profile_t1),distancia_manhattan(profile_proust,profile_t2),distancia_manhattan(profile_proust,profile_t3),distancia_manhattan(profile_proust,profile_t4),distancia_manhattan(profile_proust,profile_t5)]\n",
    "distances_austen=[distancia_manhattan(profile_austen,profile_t1),distancia_manhattan(profile_austen,profile_t2),distancia_manhattan(profile_austen,profile_t3),distancia_manhattan(profile_austen,profile_t4),distancia_manhattan(profile_austen,profile_t5)]\n",
    "distances_flauber=[distancia_manhattan(profile_flaubert,profile_t1),distancia_manhattan(profile_flaubert,profile_t2),distancia_manhattan(profile_flaubert,profile_t3),distancia_manhattan(profile_flaubert,profile_t4),distancia_manhattan(profile_flaubert,profile_t5)]\n",
    "\n",
    "a = np.array([distances_conrad,distances_zola,distances_proust,distances_austen,distances_flauber])\n",
    "df=pd.DataFrame(a,columns=['conrad','zola','proust','austen','flauber'],index=['T1','T2','T3','T4','T5'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces para simplificar las clasificaciones para nuestras instancias a determinar son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_close(conrad,zola,proust,austen,flaubert,x):\n",
    "    dist=[]\n",
    "    dist.append(distancia_manhattan(conrad,x))\n",
    "    dist.append(distancia_manhattan(zola,x))\n",
    "    dist.append(distancia_manhattan(proust,x))\n",
    "    dist.append(distancia_manhattan(austen,x))\n",
    "    dist.append(distancia_manhattan(flaubert,x))\n",
    "    if dist.index(min(dist)) == 0:\n",
    "        return 'Conrad'\n",
    "    elif dist.index(min(dist)) == 1:\n",
    "        return 'Zola'\n",
    "    elif dist.index(min(dist)) == 2:\n",
    "        return 'Proust'\n",
    "    elif dist.index(min(dist)) == 3:\n",
    "        return 'Austen'\n",
    "    elif dist.index(min(dist)) == 4:\n",
    "        return 'Flauber'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the instance t1,most close:\n",
      "Conrad\n",
      "For the instance t2,most close:\n",
      "Zola\n",
      "For the instance t3,most close:\n",
      "Proust\n",
      "For the instance t4,most close:\n",
      "Austen\n",
      "For the instance t5,most close:\n",
      "Flauber\n"
     ]
    }
   ],
   "source": [
    "print('For the instance t1,most close:')\n",
    "print(most_close(profile_conrad,profile_zola,profile_proust,profile_austen,profile_flaubert,profile_t1))\n",
    "\n",
    "print('For the instance t2,most close:')\n",
    "print(most_close(profile_conrad,profile_zola,profile_proust,profile_austen,profile_flaubert,profile_t2))\n",
    "\n",
    "print('For the instance t3,most close:')\n",
    "print(most_close(profile_conrad,profile_zola,profile_proust,profile_austen,profile_flaubert,profile_t3))\n",
    "\n",
    "print('For the instance t4,most close:')\n",
    "print(most_close(profile_conrad,profile_zola,profile_proust,profile_austen,profile_flaubert,profile_t4))\n",
    "\n",
    "print('For the instance t5,most close:')\n",
    "print(most_close(profile_conrad,profile_zola,profile_proust,profile_austen,profile_flaubert,profile_t5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte C - Atribución de autoría con Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta última sección se decidió probar con distintos enfoques para analizar el problema de la atribución de autoría, los cuales detallaremos a continuación, mostrando los resultados obtenidos y las métricas utilizadas para evaluar cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos resultó interesante comenzar el análisis utilizando algunos modelos conocidos de redes neuronales, partiendo de modelos simples y entrando también en algunos más sofisticados como son redes neuronales LSTM, Bideccionales, Convolucionales, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este análisis puede verse como un puntapié inicial de experimentación, ya que no detallaremos a fondo los resultados obtenidos utilizando redes neuronales, simplemente planteamos un acercamiento a las mismas, para luego resaltar los otros enfoques utilizados y mostrar los resultados obtenidos para los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada modelo fue entrenado con el 80% de los datos fragmentadas provistos para esta sección, formando cada instancia con el texto de un fragmento y su autor, siendo los mismos valores en el rango de 0 a 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de establecidas tanto las instancias de entrenamiento como las de prueba, se trabajó con los modelos de redes mencionados. Cada modelo de redes neuronales contiene capas distintas, y utilizada distintos parámetros como son las épocas, batchs, cantidad de neuronas de capas ocultas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detallamos a continuación los 5 modelos de redes utilizados y los correspondientes hiperparámetros correspondiente a cada modelo:\n",
    "\n",
    "* **Modelo Simple:** *Cantidad de épocas.*\n",
    "* **Modelo LSTM:** *Cantidad de épocas, tamaño \"batches\", cantidad neuronas capa LSTM y porcentaje dropout.* \n",
    "* **Modelo LSTM con capa SpartialDropout1D:** *Cantidad de épocas, tamaño \"batches\", cantidad neuronas capa LSTM y porcentaje dropout.*\n",
    "* **Modelo LSTM bidireccional:** *Cantidad de épocas, tamaño \"batches\", cantidad neuronas capa LSTM y porcentaje dropout.*\n",
    "* **Modelo Convolucional:** *Cantidad de épocas, tamaño \"batches\", cantidad neuronas capa Conv1D y porcentaje dropout.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los modelos coinciden en 2 aspectos centrales, que son el de tener una primer capa de Embeddings, y una capa final con 5 neuronas de salida, correspondientes a las posibles clases de los autores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para formar la capa de embeddings, inicialmente se tokeniza el texto de cada instancia y se genera una matriz de embeddings donde por cada palabra del vocabulario encontrada se coloca su vector de embeddings como columna de la matriz, para luego ser utilizada como primer capa de cada red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con embeddings se utilizó \"fasttext\", librería que provee vectores para una cantidad considerable de palabras en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset\n",
      "\n",
      "                                             Paragraph  Author\n",
      "0    Prefacio del autor\\nEl origen de El agente sec...       0\n",
      "1    Las sugerencias para ciertos personajes del re...       0\n",
      "2    Ocurrió que Mr. Verloc estaba dispuesto a hace...       0\n",
      "3    —Tengo aquí algunos de sus informes —dijo el f...       0\n",
      "4    —¡Agente! —dijo Mr. Verloc, sin más esfuerzo q...       0\n",
      "..                                                 ...     ...\n",
      "515  Y de nuevo corrió hacia el capitán.\\nÉste regr...       4\n",
      "516  Luego, cien pasos más adelante, volvió a deten...       4\n",
      "517  Al ver a Emma, pareció aliviado de un gran pes...       4\n",
      "518  Con muchos recuerdos.\\nVuestro padre que os qu...       4\n",
      "519  Lo cual no impidió que, cinco días más tarde, ...       4\n",
      "\n",
      "[520 rows x 2 columns]\n",
      "Loaded 13082 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:33.165424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-30 20:46:33.165485: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-07-30 20:46:40.366673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-30 20:46:40.366699: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-30 20:46:40.366719: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ale-VivoBook-S15-X510UF): /proc/driver/nvidia/version does not exist\n",
      "2021-07-30 20:46:40.366949: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20632, 300)        17325300  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6189600)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 30948005  \n",
      "=================================================================\n",
      "Total params: 48,273,305\n",
      "Trainable params: 30,948,005\n",
      "Non-trainable params: 17,325,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:41.280476: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-30 20:46:41.305283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:51.077442: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 792268800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/17 [>.............................] - ETA: 2:52 - loss: 1.6019 - sparse_categorical_accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:52.151070: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 792268800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 2/17 [==>...........................] - ETA: 10s - loss: 5.1000 - sparse_categorical_accuracy: 0.2812 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:52.859006: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 792268800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3/17 [====>.........................] - ETA: 9s - loss: 7.1209 - sparse_categorical_accuracy: 0.2639 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:53.477564: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 792268800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 4/17 [======>.......................] - ETA: 8s - loss: 8.2205 - sparse_categorical_accuracy: 0.2604"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:46:54.127849: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 792268800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 21s 630ms/step - loss: 9.6707 - sparse_categorical_accuracy: 0.2628\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 10s 570ms/step - loss: 0.2392 - sparse_categorical_accuracy: 0.9399\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 9s 540ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 8s 497ms/step - loss: 3.2624e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 9s 505ms/step - loss: 1.8144e-04 - sparse_categorical_accuracy: 1.0000\n",
      "val files..\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 2.3384 - sparse_categorical_accuracy: 0.4560\n"
     ]
    }
   ],
   "source": [
    "%run ./parte_3_redes/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este enfoque de redes neuronales, trabajamos con la métrica \"sparse_categorical_accuracy\", provista por los modelos de keras utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entraremos en detalles de los resultados obtenidos con cada modelo, ya que para utilizar estas redes, hace falta poner foco en la validación de cada hiperparámetro posible, para así seleccionar los valores adecuados e inicializar cada modelo de manera correcta, tarea que habitualmente conlleva un tiempo considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación detallaremos los otros enfoques implementados, con sus correspondientes resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar debemos importar e instalar las librerías correspondientes del siguiente bloque de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ale/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este enfoque también se utilizo el 80% de los datos como instancias de entrenamiento, y el 20% restante como dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se definen y utilizan un par de parámetros y de funciones auxiliares para leer los archivos y transformarlos en instancias con sus respectivas clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_dataset_files(dataset_path):\n",
    "    all_texts_with_classes = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        filenames.append(filename)\n",
    "        file_path = dataset_path + filename\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                text = file.read()\n",
    "                author = re.search(\"\\D+(?=\\d+\\.txt)\", filename).group(0)\n",
    "                all_texts_with_classes.append((text, author))\n",
    "    return all_texts_with_classes, filenames\n",
    "\n",
    "def extract(texts_with_classes):\n",
    "    all_texts = [t[0] for t in texts_with_classes]\n",
    "    classes = [t[1] for t in texts_with_classes]\n",
    "    all_text = \"\".join([t[0] for t in texts_with_classes])\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "    return all_texts, classes, all_tokens\n",
    "\n",
    "\n",
    "TRAINING_PROPORTION = 0.8\n",
    "DATASET_PATH = \"dataset/\"\n",
    "\n",
    "all_texts_with_classes, _ = read_dataset_files(DATASET_PATH)\n",
    "training_texts = all_texts_with_classes[:int(TRAINING_PROPORTION * len(all_texts_with_classes))]\n",
    "all_texts, classes, all_tokens = extract(training_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extrae cada palabra de las instancias como token, y se utiliza la librería \"nltk.FreqDist\" para contar las ocurrencias de cada token y posteriormente seleccionar los más frecuentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente con la librería \"CountVectorizer\" de sklearn, se construye una matriz en la que se indica para cada token, la cantidad de ocurrencias del mismo en cada instancia del dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144. 122.  99. ...   1.   0.   0.]\n",
      " [115.  84.  96. ...   2.   0.   0.]\n",
      " [151. 113. 104. ...   4.   1.   1.]\n",
      " ...\n",
      " [197.  97.  72. ...   4.   0.   0.]\n",
      " [199. 110.  73. ...   1.   1.   3.]\n",
      " [138. 136. 101. ...   3.   0.   2.]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from typing import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TOP_WORDS_BOW = 150\n",
    "\n",
    "\n",
    "def get_bow_features(tokens, texts, top_words_amount=TOP_WORDS_BOW, normalize=False):\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    vocab = [v[0] for v in sorted(fdist.items(), key=lambda x: x[1], reverse=True)][:top_words_amount]\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(texts).toarray().astype(np.float64)\n",
    "    if normalize:\n",
    "        fvs_bow = normalize_features(fvs_bow)\n",
    "    return fvs_bow\n",
    "  \n",
    "def normalize_features(features):\n",
    "    return features / np.c_[np.apply_along_axis(np.linalg.norm, 1, features)]\n",
    "\n",
    "\n",
    "fvs = OrderedDict()\n",
    "bow = get_bow_features(all_tokens, all_texts)\n",
    "print(bow)\n",
    "fvs['bow'] = normalize_features(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo pos tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con este enfoque se realiza el mismo preprocesamiento de datos mencionado en los enfoques anteriores, tomando el 80% del dataset como datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente se establece un conjunto de tags de \"part of speech\", y para cada texto del dataset, se genera un vector que indica cuantas ocurrencias del tag se encuentran en cada instancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "def get_pos_features(texts, normalize=False):\n",
    "    texts_pos = [[p[1] for p in nltk.pos_tag(nltk.word_tokenize(text))] for text in texts]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[text.count(pos) for pos in pos_list] for text in texts_pos]).astype(np.float64)\n",
    "    if normalize:\n",
    "        fvs_syntax = normalize_features(fvs_syntax)\n",
    "    return fvs_syntax  \n",
    " \n",
    "def normalize_features(features):\n",
    "    return features / np.c_[np.apply_along_axis(np.linalg.norm, 1, features)]\n",
    "\n",
    "\n",
    "pos = get_pos_features(all_texts)\n",
    "print(pos)\n",
    "fvs['pos'] = normalize_features(pos)\n",
    "fvs['both'] = normalize_features(np.c_[bow, pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos utilizados, métricas y resultados obtenidos para cada enfoque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, para cada posible representación de instancias en features (bag of words, POS tagging, y ambas funsionadas) se procede a entrenar en base a tres clasificadores distintos, todos provistos por la librería \"sklearn\". \n",
    "Detallamos a continuación los 3 clasificadores utilizados:\n",
    "\n",
    "* **Clasificador Lineal SVM:** *Clasificador basado en el concepto de Support Vector Machine.*\n",
    "* **Clasificador Gaussiano Naive Bayes:** *Clasificador probabilístico sencillo fundamentado en el teorema de Bayes, y en base a la probabilidad de una instancia dada a pertenecer a una clase, predice su valor.* \n",
    "* **Clasificador KNN:** *Clasificador que calcula los \"k\" vecinos más cercanos para cada instancia, y en base a la clasificación de sus vecinos, predice la clasificación de la instancia*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presenta a continuación la ejecución de los tres clasificadores mencionados, con los correspondientes resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se deben generar las instancias del conjunto de test, en su representación en features. Para esto se utilizan las funciones auxiliares definidas y utilizadas anteriormente para el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = all_texts_with_classes[:int((1 - TRAINING_PROPORTION) * len(all_texts_with_classes))]\n",
    "all_texts_test, test_classes_test, all_tokens_test = extract(test_texts)\n",
    "\n",
    "fvs_test = OrderedDict()\n",
    "bow = get_bow_features(all_tokens_test, all_texts_test)\n",
    "pos = get_pos_features(all_texts_test)\n",
    "fvs_test['bow'] = normalize_features(bow)\n",
    "fvs_test['pos'] = normalize_features(pos)\n",
    "fvs_test['both'] = normalize_features(np.c_[bow, pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se realiza el entrenamiento y la predicción de los autores del conjunto de test. Para todos los resultados se utiliza el accuracy como medida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def train_and_predict(classifier, x_train, y_train, x_test, y_test):\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier.score(x_test, y_test)\n",
    "\n",
    "\n",
    "results = OrderedDict()\n",
    "for feature_type in fvs.keys():\n",
    "    results[feature_type] = [\n",
    "        \"{0:.2f}%\".format(train_and_predict(LinearSVC(), fvs[feature_type], classes, fvs_test[feature_type], test_classes_test) * 100),\n",
    "        \"{0:.2f}%\".format(train_and_predict(GaussianNB(), fvs[feature_type], classes, fvs_test[feature_type], test_classes_test) * 100),\n",
    "        \"{0:.2f}%\".format(train_and_predict(KNeighborsClassifier(), fvs[feature_type], classes, fvs_test[feature_type], test_classes_test) * 100)\n",
    "    ]\n",
    "\n",
    "dfObj = pd.DataFrame.from_dict(results, orient='index', columns=['SVM', 'NB', 'KNN']) \n",
    "print(dfObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la tabla final de comparaciones, se logran obtener resultados muy buenos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primera conclusión general, se puede decir que Naive Bayes fue el modelo que arrojó peores resultados. Esto podría tener parte de su justificación en el hecho de que dicho modelo asume que existe independencia entre los atributos de las instancias, lo puede hacer que se esté perdiendo información en el caso de nuestras representaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte resulta interesante ver que la representación por bag of words es la que realmente empujó a los modelos a obtener resultados buenos.\n",
    "\n",
    "En dicha representación, cada instancia es representada por un vector con las cantidades de apariciones de las palabras más frecuentes de todo el texto, lo cual es una aplicación práctica del paper de Burrows, 2002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación por POS tags no obtuvo muy buenos resultados por sí sola, pero ayudó a mejorar el accuracy al utilizarla en conjunto con la representación por bag of words en un 5% aproximadamente para el caso de KNN, llegando a un valor de **97.66%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conculsiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ley de Zipf y corrección de Zipf-Mandelbrot http://statweb.stanford.edu/~owen/courses/306a/ZipfAndGutenberg.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
